{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Approche neuronale "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7fcb89e93deb657"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Étape 1 : Chargement et Préparation des Données avec PyTorch"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3e9dc92c35432651"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Trie des labels"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6d2962ef94d1dc8e"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Il y a 10177 labels uniques dans le fichier.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Remplacez 'path_to_file.txt' par le chemin réel vers votre fichier\n",
    "file_path = 'Image/Projet IAI3/identity_CelebA.txt'\n",
    "\n",
    "# Lire le fichier en supposant qu'il est séparé par des espaces\n",
    "data = pd.read_csv(file_path, delim_whitespace=True, header=None, names=['filename', 'label'])\n",
    "\n",
    "# Calculer le nombre de labels uniques\n",
    "unique_labels_count = data['label'].nunique()\n",
    "\n",
    "print(f'Il y a {unique_labels_count} labels uniques dans le fichier.')\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-19T13:22:32.196226500Z",
     "start_time": "2023-12-19T13:22:31.984475200Z"
    }
   },
   "id": "a3797a7323103c26"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Remplacez 'your_file.txt' par le chemin d'accès à votre fichier réel\n",
    "file_path = 'Image/Projet IAI3/identity_CelebA.txt'\n",
    "\n",
    "# Lire le fichier\n",
    "data = pd.read_csv(file_path, delim_whitespace=True, header=None, names=['filename', 'label'])\n",
    "\n",
    "# Grouper les fichiers par label\n",
    "grouped = data.groupby('label')['filename'].apply(list).to_dict()\n",
    "\n",
    "# Créer une structure JSON\n",
    "json_structure = {'data': grouped}\n",
    "\n",
    "# Pour écrire dans un fichier JSON\n",
    "with open('sorted_by_label.json', 'w') as json_file:\n",
    "    json.dump(json_structure, json_file, indent=4)\n",
    "\n",
    "# Afficher la structure JSON si nécessaire\n",
    "#print(json.dumps(json_structure, indent=4))\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-19T13:29:47.743301300Z",
     "start_time": "2023-12-19T13:29:47.093129700Z"
    }
   },
   "id": "33bfc829c3c87dc2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-12-19T13:21:41.241762100Z",
     "start_time": "2023-12-19T13:21:41.241762100Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "\n",
    "# Définir les transformations, cela pourrait inclure la normalisation, le redimensionnement, etc.\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# Charger les données de CelebA\n",
    "# Vous devrez éventuellement ajuster les chemins et autres paramètres en fonction de votre organisation de fichiers\n",
    "celeba_data = datasets.ImageFolder(root='Image/Projet IAI3/img_align_celeba', transform=transform)\n",
    "\n",
    "# Diviser les données en ensembles d'entraînement, de validation et de test en utilisant les indices fournis par CelebA\n",
    "train_indices = [...] # Liste des indices pour l'entraînement\n",
    "val_indices = [...]   # Liste des indices pour la validation\n",
    "test_indices = [...]  # Liste des indices pour le test\n",
    "\n",
    "train_data = Subset(celeba_data, train_indices)\n",
    "val_data = Subset(celeba_data, val_indices)\n",
    "test_data = Subset(celeba_data, test_indices)\n",
    "\n",
    "# Créer des DataLoaders pour itérer sur les données\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_data, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Étape 2 : Construire le Modèle avec PyTorch"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "716a62a1073ad3d0"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Définir un modèle simple CNN avec PyTorch\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, 3, padding=1) # L'input doit correspondre au nombre de canaux de l'image\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.fc1 = nn.Linear(64 * 54 * 44, 512) # La taille doit correspondre à la sortie de la dernière couche MaxPool\n",
    "        self.fc2 = nn.Linear(512, 1) # ou le nombre de classes pour la classification multi-classes\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 64 * 54 * 44) # Aplatir les sorties pour les connecter à la couche dense\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = torch.sigmoid(self.fc2(x)) # ou F.log_softmax pour la classification multi-classes\n",
    "        return x\n",
    "\n",
    "model = SimpleCNN()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-19T13:06:57.627574100Z",
     "start_time": "2023-12-19T13:06:57.281234500Z"
    }
   },
   "id": "9d22c144d5723f75"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Etape 3 : Boucle d'entrainement\n",
    "Cela inclut l'entraînement du modèle, le calcul de la perte, la mise à jour des poids du modèle, et la validation à la fin de chaque époque."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f9ace0165bde1bc9"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not ellipsis",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[3], line 16\u001B[0m\n\u001B[0;32m     13\u001B[0m model\u001B[38;5;241m.\u001B[39mtrain()  \u001B[38;5;66;03m# Mettre le modèle en mode entraînement\u001B[39;00m\n\u001B[0;32m     14\u001B[0m running_loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0.0\u001B[39m\n\u001B[1;32m---> 16\u001B[0m \u001B[43m\u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mi\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdata\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43menumerate\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mtrain_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m:\u001B[49m\n\u001B[0;32m     17\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;66;43;03m# obtenir les entrées ; data est une liste de [inputs, labels]\u001B[39;49;00m\n\u001B[0;32m     18\u001B[0m \u001B[43m    \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlabels\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mdata\u001B[49m\n\u001B[0;32m     20\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;66;43;03m# zéro les gradients des paramètres\u001B[39;49;00m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:630\u001B[0m, in \u001B[0;36m_BaseDataLoaderIter.__next__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    627\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sampler_iter \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    628\u001B[0m     \u001B[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001B[39;00m\n\u001B[0;32m    629\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reset()  \u001B[38;5;66;03m# type: ignore[call-arg]\u001B[39;00m\n\u001B[1;32m--> 630\u001B[0m data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_next_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    631\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m    632\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_dataset_kind \u001B[38;5;241m==\u001B[39m _DatasetKind\u001B[38;5;241m.\u001B[39mIterable \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[0;32m    633\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[0;32m    634\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m>\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called:\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:674\u001B[0m, in \u001B[0;36m_SingleProcessDataLoaderIter._next_data\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    672\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_next_data\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    673\u001B[0m     index \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_next_index()  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[1;32m--> 674\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_dataset_fetcher\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfetch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mindex\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[0;32m    675\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory:\n\u001B[0;32m    676\u001B[0m         data \u001B[38;5;241m=\u001B[39m _utils\u001B[38;5;241m.\u001B[39mpin_memory\u001B[38;5;241m.\u001B[39mpin_memory(data, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory_device)\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:49\u001B[0m, in \u001B[0;36m_MapDatasetFetcher.fetch\u001B[1;34m(self, possibly_batched_index)\u001B[0m\n\u001B[0;32m     47\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mauto_collation:\n\u001B[0;32m     48\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m__getitems__\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset\u001B[38;5;241m.\u001B[39m__getitems__:\n\u001B[1;32m---> 49\u001B[0m         data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdataset\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m__getitems__\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpossibly_batched_index\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     50\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m     51\u001B[0m         data \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[idx] \u001B[38;5;28;01mfor\u001B[39;00m idx \u001B[38;5;129;01min\u001B[39;00m possibly_batched_index]\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataset.py:364\u001B[0m, in \u001B[0;36mSubset.__getitems__\u001B[1;34m(self, indices)\u001B[0m\n\u001B[0;32m    362\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset\u001B[38;5;241m.\u001B[39m__getitems__([\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mindices[idx] \u001B[38;5;28;01mfor\u001B[39;00m idx \u001B[38;5;129;01min\u001B[39;00m indices])  \u001B[38;5;66;03m# type: ignore[attr-defined]\u001B[39;00m\n\u001B[0;32m    363\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 364\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m[\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdataset\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mindices\u001B[49m\u001B[43m[\u001B[49m\u001B[43midx\u001B[49m\u001B[43m]\u001B[49m\u001B[43m]\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43midx\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mindices\u001B[49m\u001B[43m]\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataset.py:364\u001B[0m, in \u001B[0;36m<listcomp>\u001B[1;34m(.0)\u001B[0m\n\u001B[0;32m    362\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset\u001B[38;5;241m.\u001B[39m__getitems__([\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mindices[idx] \u001B[38;5;28;01mfor\u001B[39;00m idx \u001B[38;5;129;01min\u001B[39;00m indices])  \u001B[38;5;66;03m# type: ignore[attr-defined]\u001B[39;00m\n\u001B[0;32m    363\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 364\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m [\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdataset\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mindices\u001B[49m\u001B[43m[\u001B[49m\u001B[43midx\u001B[49m\u001B[43m]\u001B[49m\u001B[43m]\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m idx \u001B[38;5;129;01min\u001B[39;00m indices]\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\datasets\\folder.py:228\u001B[0m, in \u001B[0;36mDatasetFolder.__getitem__\u001B[1;34m(self, index)\u001B[0m\n\u001B[0;32m    220\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__getitem__\u001B[39m(\u001B[38;5;28mself\u001B[39m, index: \u001B[38;5;28mint\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tuple[Any, Any]:\n\u001B[0;32m    221\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    222\u001B[0m \u001B[38;5;124;03m    Args:\u001B[39;00m\n\u001B[0;32m    223\u001B[0m \u001B[38;5;124;03m        index (int): Index\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    226\u001B[0m \u001B[38;5;124;03m        tuple: (sample, target) where target is class_index of the target class.\u001B[39;00m\n\u001B[0;32m    227\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m--> 228\u001B[0m     path, target \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msamples\u001B[49m\u001B[43m[\u001B[49m\u001B[43mindex\u001B[49m\u001B[43m]\u001B[49m\n\u001B[0;32m    229\u001B[0m     sample \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mloader(path)\n\u001B[0;32m    230\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtransform \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "\u001B[1;31mTypeError\u001B[0m: list indices must be integers or slices, not ellipsis"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# Paramètres\n",
    "epochs = 5  # Nombre d'époques\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Définition de la fonction de perte et de l'optimiseur\n",
    "criterion = nn.BCELoss()  # Pour la classification binaire\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Boucle d'entraînement\n",
    "for epoch in range(epochs):\n",
    "    model.train()  # Mettre le modèle en mode entraînement\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        # obtenir les entrées ; data est une liste de [inputs, labels]\n",
    "        inputs, labels = data\n",
    "\n",
    "        # zéro les gradients des paramètres\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs.squeeze(), labels.float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Imprimer les statistiques\n",
    "        running_loss += loss.item()\n",
    "        if i % 100 == 99:    # Imprimer toutes les 100 mini-batches\n",
    "            print(f'Époque {epoch + 1}, Batch {i + 1}, Perte: {running_loss / 100:.4f}')\n",
    "            running_loss = 0.0\n",
    "\n",
    "    # Validation après chaque époque\n",
    "    model.eval()  # Mettre le modèle en mode évaluation\n",
    "    val_loss = 0.0\n",
    "    val_steps = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in val_loader:\n",
    "            images, labels = data\n",
    "            outputs = model(images)\n",
    "            predicted = (outputs.squeeze() > 0.5).float()  # Seuil de 0.5 pour la classification binaire\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            loss = criterion(outputs.squeeze(), labels.float())\n",
    "            val_loss += loss.item()\n",
    "            val_steps += 1\n",
    "\n",
    "    # Imprimer les statistiques de validation\n",
    "    print(f'Validation Loss: {val_loss / val_steps:.4f}')\n",
    "    print(f'Validation Accuracy: {100 * correct / total:.2f}%')\n",
    "\n",
    "print('Fin de l\\'entraînement')\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-19T13:06:57.982685700Z",
     "start_time": "2023-12-19T13:06:57.632387600Z"
    }
   },
   "id": "70f5924fb6998069"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
