{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Installation des librairies et vérification que CUDA est bien installé"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4f28f4a53b69f1a3"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: keras in c:\\users\\hydre\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (2.15.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.3.2 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scipy in c:\\users\\hydre\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (1.11.4)\n",
      "Requirement already satisfied: numpy<1.28.0,>=1.21.6 in c:\\users\\hydre\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from scipy) (1.23.5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.3.2 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pillow in c:\\users\\hydre\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (10.1.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.3.2 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in c:\\users\\hydre\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (2.15.0)\n",
      "Requirement already satisfied: tensorflow-intel==2.15.0 in c:\\users\\hydre\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow) (2.15.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\hydre\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (2.0.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\hydre\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in c:\\users\\hydre\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (23.5.26)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\hydre\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (0.4.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\hydre\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in c:\\users\\hydre\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (3.10.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\hydre\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (16.0.6)\n",
      "Requirement already satisfied: ml-dtypes~=0.2.0 in c:\\users\\hydre\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in c:\\users\\hydre\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (1.23.5)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\hydre\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (3.3.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\hydre\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (23.2)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in c:\\users\\hydre\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (4.23.4)\n",
      "Requirement already satisfied: setuptools in c:\\users\\hydre\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (65.5.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\hydre\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\hydre\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (2.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\hydre\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (4.8.0)\n",
      "Requirement already satisfied: wrapt<1.15,>=1.11.0 in c:\\users\\hydre\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (1.14.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\hydre\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (0.31.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\hydre\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (1.59.3)\n",
      "Requirement already satisfied: tensorboard<2.16,>=2.15 in c:\\users\\hydre\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (2.15.1)\n",
      "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in c:\\users\\hydre\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (2.15.0)\n",
      "Requirement already satisfied: keras<2.16,>=2.15.0 in c:\\users\\hydre\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (2.15.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\hydre\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.15.0->tensorflow) (0.42.0)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\users\\hydre\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (2.24.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in c:\\users\\hydre\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (1.0.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\hydre\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (3.5.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\hydre\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (2.31.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\hydre\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\hydre\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (3.0.1)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\hydre\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (5.3.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\hydre\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (0.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\hydre\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\hydre\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (1.3.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\hydre\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\hydre\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\hydre\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\hydre\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (2023.7.22)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\hydre\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (2.1.3)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in c:\\users\\hydre\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (0.5.1)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\hydre\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (3.2.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.3.2 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install keras\n",
    "!pip install scipy\n",
    "!pip install pillow\n",
    "!pip install tensorflow"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-16T23:08:57.947122100Z",
     "start_time": "2024-02-16T23:08:49.139825900Z"
    }
   },
   "id": "ce8ac5fd7c8f22e3"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'keras.backend' has no attribute 'tensorflow_backend'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[4], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mkeras\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m backend \u001B[38;5;28;01mas\u001B[39;00m K\n\u001B[1;32m----> 2\u001B[0m \u001B[43mK\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtensorflow_backend\u001B[49m\u001B[38;5;241m.\u001B[39m_get_available_gpus()\n\u001B[0;32m      4\u001B[0m \u001B[38;5;66;03m# Chemin à ajuster par rapport à votre dataset.\u001B[39;00m\n\u001B[0;32m      5\u001B[0m pathImageCelib \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m../Image/img_align_celeba/img_align_celeba\u001B[39m\u001B[38;5;124m'\u001B[39m\n",
      "\u001B[1;31mAttributeError\u001B[0m: module 'keras.backend' has no attribute 'tensorflow_backend'"
     ]
    }
   ],
   "source": [
    "from keras import backend as K\n",
    "K.tensorflow_backend._get_available_gpus()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-16T23:09:06.123530400Z",
     "start_time": "2024-02-16T23:09:06.103358800Z"
    }
   },
   "id": "e7329e6d995cfc8d"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# Chemin à ajuster par rapport à votre dataset.\n",
    "pathImageCelib = '../Image/img_align_celeba/img_align_celeba'\n",
    "\n",
    "pathTestSet = '../Image/testSet'\n",
    "pathTrainSet = '../Image/trainSet'\n",
    "pathValidationSet = '../Image/validationSet'"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-16T23:11:37.290780Z",
     "start_time": "2024-02-16T23:11:37.277279900Z"
    }
   },
   "id": "314c0adb38d90db0"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Vérification des GPUs disponible"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "99a601f657c9585e"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPUs Available:  []\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(\"GPUs Available: \", tf.config.list_physical_devices('GPU'))\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-16T23:11:42.229918Z",
     "start_time": "2024-02-16T23:11:42.219389600Z"
    }
   },
   "id": "682e44b653fc4db6"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1. Préparation des Données\n",
    "### 1.1 Organisation des Images : Rangez vos images dans une structure de dossiers appropriée, typiquement un dossier pour chaque classe (homme, femme)."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "27720ea81e14cb6b"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "000023.jpg\n",
      "Image file does not exist: ..\\Image\\img_align_celeba\\img_align_celeba\\000023.jpg\n",
      "Invalid input, skipping this label.\n",
      "016188.jpg\n",
      "Image file does not exist: ..\\Image\\img_align_celeba\\img_align_celeba\\016188.jpg\n",
      "Invalid input, skipping this label.\n",
      "003029.jpg\n",
      "Image file does not exist: ..\\Image\\img_align_celeba\\img_align_celeba\\003029.jpg\n",
      "Invalid input, skipping this label.\n",
      "008268.jpg\n",
      "Image file does not exist: ..\\Image\\img_align_celeba\\img_align_celeba\\008268.jpg\n",
      "Invalid input, skipping this label.\n",
      "000456.jpg\n",
      "Image file does not exist: ..\\Image\\img_align_celeba\\img_align_celeba\\000456.jpg\n",
      "Invalid input, skipping this label.\n",
      "002617.jpg\n",
      "Image file does not exist: ..\\Image\\img_align_celeba\\img_align_celeba\\002617.jpg\n",
      "Invalid input, skipping this label.\n",
      "002065.jpg\n",
      "Image file does not exist: ..\\Image\\img_align_celeba\\img_align_celeba\\002065.jpg\n",
      "Invalid input, skipping this label.\n",
      "003089.jpg\n",
      "Image file does not exist: ..\\Image\\img_align_celeba\\img_align_celeba\\003089.jpg\n",
      "Invalid input, skipping this label.\n",
      "010959.jpg\n",
      "Image file does not exist: ..\\Image\\img_align_celeba\\img_align_celeba\\010959.jpg\n",
      "Invalid input, skipping this label.\n",
      "002307.jpg\n",
      "Image file does not exist: ..\\Image\\img_align_celeba\\img_align_celeba\\002307.jpg\n",
      "Invalid input, skipping this label.\n",
      "063458.jpg\n",
      "Image file does not exist: ..\\Image\\img_align_celeba\\img_align_celeba\\063458.jpg\n",
      "Invalid input, skipping this label.\n",
      "002486.jpg\n",
      "Image file does not exist: ..\\Image\\img_align_celeba\\img_align_celeba\\002486.jpg\n",
      "Invalid input, skipping this label.\n",
      "000077.jpg\n",
      "Image file does not exist: ..\\Image\\img_align_celeba\\img_align_celeba\\000077.jpg\n",
      "Invalid input, skipping this label.\n",
      "032486.jpg\n",
      "Image file does not exist: ..\\Image\\img_align_celeba\\img_align_celeba\\032486.jpg\n",
      "Invalid input, skipping this label.\n",
      "011256.jpg\n",
      "Image file does not exist: ..\\Image\\img_align_celeba\\img_align_celeba\\011256.jpg\n",
      "Invalid input, skipping this label.\n",
      "016740.jpg\n",
      "Image file does not exist: ..\\Image\\img_align_celeba\\img_align_celeba\\016740.jpg\n",
      "Invalid input, skipping this label.\n",
      "003677.jpg\n",
      "Image file does not exist: ..\\Image\\img_align_celeba\\img_align_celeba\\003677.jpg\n",
      "Invalid input, skipping this label.\n",
      "162774.jpg\n",
      "Image file does not exist: ..\\Image\\img_align_celeba\\img_align_celeba\\162774.jpg\n",
      "Invalid input, skipping this label.\n",
      "004319.jpg\n",
      "Image file does not exist: ..\\Image\\img_align_celeba\\img_align_celeba\\004319.jpg\n",
      "Invalid input, skipping this label.\n",
      "004538.jpg\n",
      "Image file does not exist: ..\\Image\\img_align_celeba\\img_align_celeba\\004538.jpg\n",
      "Invalid input, skipping this label.\n",
      "003039.jpg\n",
      "Image file does not exist: ..\\Image\\img_align_celeba\\img_align_celeba\\003039.jpg\n",
      "Invalid input, skipping this label.\n",
      "000481.jpg\n",
      "Image file does not exist: ..\\Image\\img_align_celeba\\img_align_celeba\\000481.jpg\n",
      "Invalid input, skipping this label.\n",
      "004488.jpg\n",
      "Image file does not exist: ..\\Image\\img_align_celeba\\img_align_celeba\\004488.jpg\n",
      "Invalid input, skipping this label.\n",
      "001505.jpg\n",
      "Image file does not exist: ..\\Image\\img_align_celeba\\img_align_celeba\\001505.jpg\n",
      "Invalid input, skipping this label.\n",
      "005346.jpg\n",
      "Image file does not exist: ..\\Image\\img_align_celeba\\img_align_celeba\\005346.jpg\n",
      "Invalid input, skipping this label.\n",
      "002162.jpg\n",
      "Image file does not exist: ..\\Image\\img_align_celeba\\img_align_celeba\\002162.jpg\n",
      "Invalid input, skipping this label.\n",
      "001784.jpg\n",
      "Image file does not exist: ..\\Image\\img_align_celeba\\img_align_celeba\\001784.jpg\n",
      "Invalid input, skipping this label.\n",
      "000835.jpg\n",
      "Image file does not exist: ..\\Image\\img_align_celeba\\img_align_celeba\\000835.jpg\n",
      "Invalid input, skipping this label.\n",
      "004379.jpg\n",
      "Image file does not exist: ..\\Image\\img_align_celeba\\img_align_celeba\\004379.jpg\n",
      "Invalid input, skipping this label.\n",
      "020646.jpg\n",
      "Image file does not exist: ..\\Image\\img_align_celeba\\img_align_celeba\\020646.jpg\n",
      "Invalid input, skipping this label.\n",
      "009309.jpg\n",
      "Image file does not exist: ..\\Image\\img_align_celeba\\img_align_celeba\\009309.jpg\n",
      "Invalid input, skipping this label.\n",
      "000605.jpg\n",
      "Image file does not exist: ..\\Image\\img_align_celeba\\img_align_celeba\\000605.jpg\n",
      "Invalid input, skipping this label.\n",
      "000492.jpg\n",
      "Image file does not exist: ..\\Image\\img_align_celeba\\img_align_celeba\\000492.jpg\n",
      "Invalid input, skipping this label.\n",
      "011199.jpg\n",
      "Image file does not exist: ..\\Image\\img_align_celeba\\img_align_celeba\\011199.jpg\n",
      "Invalid input, skipping this label.\n",
      "001265.jpg\n",
      "Image file does not exist: ..\\Image\\img_align_celeba\\img_align_celeba\\001265.jpg\n",
      "Invalid input, skipping this label.\n",
      "021995.jpg\n",
      "Image file does not exist: ..\\Image\\img_align_celeba\\img_align_celeba\\021995.jpg\n",
      "Invalid input, skipping this label.\n",
      "002329.jpg\n",
      "Image file does not exist: ..\\Image\\img_align_celeba\\img_align_celeba\\002329.jpg\n",
      "Invalid input, skipping this label.\n",
      "028286.jpg\n",
      "Image file does not exist: ..\\Image\\img_align_celeba\\img_align_celeba\\028286.jpg\n",
      "Invalid input, skipping this label.\n",
      "002270.jpg\n",
      "Image file does not exist: ..\\Image\\img_align_celeba\\img_align_celeba\\002270.jpg\n",
      "Invalid input, skipping this label.\n",
      "009551.jpg\n",
      "Image file does not exist: ..\\Image\\img_align_celeba\\img_align_celeba\\009551.jpg\n",
      "Invalid input, skipping this label.\n",
      "010700.jpg\n",
      "Image file does not exist: ..\\Image\\img_align_celeba\\img_align_celeba\\010700.jpg\n",
      "Invalid input, skipping this label.\n",
      "007906.jpg\n",
      "Image file does not exist: ..\\Image\\img_align_celeba\\img_align_celeba\\007906.jpg\n",
      "Invalid input, skipping this label.\n",
      "004683.jpg\n",
      "Image file does not exist: ..\\Image\\img_align_celeba\\img_align_celeba\\004683.jpg\n",
      "Invalid input, skipping this label.\n",
      "040169.jpg\n",
      "Image file does not exist: ..\\Image\\img_align_celeba\\img_align_celeba\\040169.jpg\n",
      "Invalid input, skipping this label.\n",
      "005122.jpg\n",
      "Image file does not exist: ..\\Image\\img_align_celeba\\img_align_celeba\\005122.jpg\n",
      "Invalid input, skipping this label.\n",
      "004234.jpg\n",
      "Image file does not exist: ..\\Image\\img_align_celeba\\img_align_celeba\\004234.jpg\n",
      "Invalid input, skipping this label.\n",
      "002733.jpg\n",
      "Image file does not exist: ..\\Image\\img_align_celeba\\img_align_celeba\\002733.jpg\n",
      "Invalid input, skipping this label.\n",
      "012555.jpg\n",
      "Image file does not exist: ..\\Image\\img_align_celeba\\img_align_celeba\\012555.jpg\n",
      "Invalid input, skipping this label.\n",
      "002550.jpg\n",
      "Image file does not exist: ..\\Image\\img_align_celeba\\img_align_celeba\\002550.jpg\n",
      "Invalid input, skipping this label.\n",
      "007145.jpg\n",
      "Image file does not exist: ..\\Image\\img_align_celeba\\img_align_celeba\\007145.jpg\n",
      "Invalid input, skipping this label.\n",
      "008964.jpg\n",
      "Image file does not exist: ..\\Image\\img_align_celeba\\img_align_celeba\\008964.jpg\n",
      "Invalid input, skipping this label.\n",
      "010653.jpg\n",
      "Image file does not exist: ..\\Image\\img_align_celeba\\img_align_celeba\\010653.jpg\n",
      "Invalid input, skipping this label.\n",
      "001171.jpg\n",
      "Image file does not exist: ..\\Image\\img_align_celeba\\img_align_celeba\\001171.jpg\n",
      "Invalid input, skipping this label.\n",
      "001981.jpg\n",
      "Image file does not exist: ..\\Image\\img_align_celeba\\img_align_celeba\\001981.jpg\n",
      "Invalid input, skipping this label.\n",
      "013204.jpg\n",
      "Image file does not exist: ..\\Image\\img_align_celeba\\img_align_celeba\\013204.jpg\n",
      "Invalid input, skipping this label.\n",
      "163136.jpg\n",
      "Image file does not exist: ..\\Image\\img_align_celeba\\img_align_celeba\\163136.jpg\n",
      "Invalid input, skipping this label.\n",
      "004198.jpg\n",
      "Image file does not exist: ..\\Image\\img_align_celeba\\img_align_celeba\\004198.jpg\n",
      "Invalid input, skipping this label.\n",
      "012500.jpg\n",
      "Image file does not exist: ..\\Image\\img_align_celeba\\img_align_celeba\\012500.jpg\n",
      "Invalid input, skipping this label.\n",
      "000129.jpg\n",
      "Image file does not exist: ..\\Image\\img_align_celeba\\img_align_celeba\\000129.jpg\n",
      "Invalid input, skipping this label.\n",
      "023854.jpg\n",
      "Image file does not exist: ..\\Image\\img_align_celeba\\img_align_celeba\\023854.jpg\n",
      "Invalid input, skipping this label.\n",
      "001352.jpg\n",
      "Image file does not exist: ..\\Image\\img_align_celeba\\img_align_celeba\\001352.jpg\n",
      "Invalid input, skipping this label.\n",
      "163164.jpg\n",
      "Image file does not exist: ..\\Image\\img_align_celeba\\img_align_celeba\\163164.jpg\n",
      "Invalid input, skipping this label.\n",
      "016486.jpg\n",
      "Image file does not exist: ..\\Image\\img_align_celeba\\img_align_celeba\\016486.jpg\n",
      "Invalid input, skipping this label.\n",
      "011839.jpg\n",
      "Image file does not exist: ..\\Image\\img_align_celeba\\img_align_celeba\\011839.jpg\n",
      "Invalid input, skipping this label.\n",
      "000854.jpg\n",
      "Image file does not exist: ..\\Image\\img_align_celeba\\img_align_celeba\\000854.jpg\n",
      "Invalid input, skipping this label.\n",
      "002438.jpg\n",
      "Image file does not exist: ..\\Image\\img_align_celeba\\img_align_celeba\\002438.jpg\n",
      "Invalid input, skipping this label.\n",
      "000932.jpg\n",
      "Image file does not exist: ..\\Image\\img_align_celeba\\img_align_celeba\\000932.jpg\n",
      "Invalid input, skipping this label.\n",
      "016684.jpg\n",
      "Image file does not exist: ..\\Image\\img_align_celeba\\img_align_celeba\\016684.jpg\n",
      "Invalid input, skipping this label.\n",
      "001593.jpg\n",
      "Image file does not exist: ..\\Image\\img_align_celeba\\img_align_celeba\\001593.jpg\n",
      "Invalid input, skipping this label.\n",
      "021779.jpg\n",
      "Image file does not exist: ..\\Image\\img_align_celeba\\img_align_celeba\\021779.jpg\n",
      "Invalid input, skipping this label.\n",
      "009590.jpg\n",
      "Image file does not exist: ..\\Image\\img_align_celeba\\img_align_celeba\\009590.jpg\n",
      "Invalid input, skipping this label.\n",
      "022783.jpg\n",
      "Image file does not exist: ..\\Image\\img_align_celeba\\img_align_celeba\\022783.jpg\n",
      "Invalid input, skipping this label.\n",
      "001745.jpg\n",
      "Image file does not exist: ..\\Image\\img_align_celeba\\img_align_celeba\\001745.jpg\n",
      "Invalid input, skipping this label.\n",
      "165513.jpg\n",
      "Image file does not exist: ..\\Image\\img_align_celeba\\img_align_celeba\\165513.jpg\n",
      "Invalid input, skipping this label.\n",
      "004466.jpg\n",
      "Image file does not exist: ..\\Image\\img_align_celeba\\img_align_celeba\\004466.jpg\n",
      "Invalid input, skipping this label.\n",
      "170596.jpg\n",
      "Image file does not exist: ..\\Image\\img_align_celeba\\img_align_celeba\\170596.jpg\n",
      "Invalid input, skipping this label.\n",
      "013306.jpg\n",
      "Image file does not exist: ..\\Image\\img_align_celeba\\img_align_celeba\\013306.jpg\n",
      "Invalid input, skipping this label.\n",
      "000171.jpg\n",
      "Image file does not exist: ..\\Image\\img_align_celeba\\img_align_celeba\\000171.jpg\n",
      "Invalid input, skipping this label.\n",
      "004556.jpg\n",
      "Image file does not exist: ..\\Image\\img_align_celeba\\img_align_celeba\\004556.jpg\n",
      "Invalid input, skipping this label.\n",
      "010998.jpg\n",
      "Image file does not exist: ..\\Image\\img_align_celeba\\img_align_celeba\\010998.jpg\n",
      "Invalid input, skipping this label.\n",
      "006514.jpg\n",
      "Image file does not exist: ..\\Image\\img_align_celeba\\img_align_celeba\\006514.jpg\n",
      "Invalid input, skipping this label.\n",
      "005373.jpg\n",
      "Image file does not exist: ..\\Image\\img_align_celeba\\img_align_celeba\\005373.jpg\n",
      "Invalid input, skipping this label.\n",
      "004643.jpg\n",
      "Image file does not exist: ..\\Image\\img_align_celeba\\img_align_celeba\\004643.jpg\n",
      "Invalid input, skipping this label.\n",
      "006901.jpg\n",
      "Image file does not exist: ..\\Image\\img_align_celeba\\img_align_celeba\\006901.jpg\n",
      "Invalid input, skipping this label.\n",
      "007222.jpg\n",
      "Image file does not exist: ..\\Image\\img_align_celeba\\img_align_celeba\\007222.jpg\n",
      "Invalid input, skipping this label.\n",
      "001908.jpg\n",
      "Image file does not exist: ..\\Image\\img_align_celeba\\img_align_celeba\\001908.jpg\n",
      "Invalid input, skipping this label.\n",
      "000567.jpg\n",
      "Image file does not exist: ..\\Image\\img_align_celeba\\img_align_celeba\\000567.jpg\n",
      "Invalid input, skipping this label.\n",
      "006183.jpg\n",
      "Image file does not exist: ..\\Image\\img_align_celeba\\img_align_celeba\\006183.jpg\n",
      "Invalid input, skipping this label.\n",
      "002409.jpg\n",
      "Image file does not exist: ..\\Image\\img_align_celeba\\img_align_celeba\\002409.jpg\n",
      "Invalid input, skipping this label.\n",
      "008691.jpg\n",
      "Image file does not exist: ..\\Image\\img_align_celeba\\img_align_celeba\\008691.jpg\n",
      "Invalid input, skipping this label.\n",
      "002204.jpg\n",
      "Image file does not exist: ..\\Image\\img_align_celeba\\img_align_celeba\\002204.jpg\n",
      "Invalid input, skipping this label.\n",
      "007494.jpg\n",
      "Image file does not exist: ..\\Image\\img_align_celeba\\img_align_celeba\\007494.jpg\n",
      "Invalid input, skipping this label.\n",
      "021648.jpg\n",
      "Image file does not exist: ..\\Image\\img_align_celeba\\img_align_celeba\\021648.jpg\n",
      "Invalid input, skipping this label.\n",
      "005110.jpg\n",
      "Image file does not exist: ..\\Image\\img_align_celeba\\img_align_celeba\\005110.jpg\n",
      "Invalid input, skipping this label.\n",
      "011073.jpg\n",
      "Image file does not exist: ..\\Image\\img_align_celeba\\img_align_celeba\\011073.jpg\n",
      "Invalid input, skipping this label.\n",
      "004849.jpg\n",
      "Image file does not exist: ..\\Image\\img_align_celeba\\img_align_celeba\\004849.jpg\n",
      "Invalid input, skipping this label.\n",
      "162839.jpg\n",
      "Image file does not exist: ..\\Image\\img_align_celeba\\img_align_celeba\\162839.jpg\n",
      "Invalid input, skipping this label.\n",
      "005390.jpg\n",
      "Image file does not exist: ..\\Image\\img_align_celeba\\img_align_celeba\\005390.jpg\n",
      "Invalid input, skipping this label.\n",
      "021720.jpg\n",
      "Image file does not exist: ..\\Image\\img_align_celeba\\img_align_celeba\\021720.jpg\n",
      "Invalid input, skipping this label.\n",
      "163099.jpg\n",
      "Image file does not exist: ..\\Image\\img_align_celeba\\img_align_celeba\\163099.jpg\n",
      "Invalid input, skipping this label.\n",
      "Classification complete! Results saved to classification_result.json.\n"
     ]
    }
   ],
   "source": [
    "# Les fichiers sont déjà disponible afin de vous éviter de devoir classer des images.\n",
    "# Les fichiers :\n",
    "# - Phase 2.1 - Apprentissage profond/classification_result-trainSet.json\n",
    "# - Phase 2.1 - Apprentissage profond/classification_result-testSet.json\n",
    "# - Phase 2.1 - Apprentissage profond/classification_result-validationSet.json\n",
    "\n",
    "# 1. Préparation des Données\n",
    "# Organisation des Images : Rangez vos images dans une structure de dossiers appropriée, typiquement un dossier pour chaque classe (homme, femme).\n",
    "\n",
    "# classifier.py\n",
    "# Permet de classifier les images (homme ou femme) afin de créer des données.\n",
    "\n",
    "\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import cv2 as cv\n",
    "\n",
    "\n",
    "# Chemin vers votre fichier JSON\n",
    "fichier_json = '../Data/sorted_by_label.json'\n",
    "\n",
    "# Charger les vecteurs moyens à partir du fichier JSON\n",
    "with open(fichier_json, 'r') as file:\n",
    "    json_content = json.load(file)\n",
    "\n",
    "# Path to the directory where images are stored\n",
    "image_directory_path = Path(pathImageCelib)\n",
    "\n",
    "# Function to display an image by filename\n",
    "def display_image(filename):\n",
    "    try:\n",
    "        image_path = image_directory_path / filename\n",
    "        if not image_path.exists():\n",
    "            print(f\"Image file does not exist: {image_path}\")\n",
    "            return\n",
    "        image = Image.open(image_path)\n",
    "        image.show()\n",
    "    except IOError as e:\n",
    "        print(f\"Error opening image {filename}: {e}\")\n",
    "\n",
    "\n",
    "# Function to classify images based on user input and create a JSON structure\n",
    "def classify_images(json_data, max_labels=None):\n",
    "    classification_result = {'men': [], 'women': []}\n",
    "    label_count = 0\n",
    "\n",
    "    for label, images in json_data['data'].items():\n",
    "        if max_labels is not None and label_count >= max_labels:\n",
    "            break  # Arrête le traitement après un certain nombre de labels\n",
    "\n",
    "        if images:  # If there are images for the label\n",
    "            # Display the first image of the label\n",
    "            print(images[0])\n",
    "            display_image(images[0])\n",
    "            # Ask the user to classify the label\n",
    "            classification = input(\"Classify the person as a man (m) or a woman (w): \").strip().lower()\n",
    "            if classification == 'm':\n",
    "                classification_result['men'].extend(images)\n",
    "            elif classification == 'w':\n",
    "                classification_result['women'].extend(images)\n",
    "            else:\n",
    "                print(\"Invalid input, skipping this label.\")\n",
    "\n",
    "        label_count += 1\n",
    "\n",
    "    # Write the classification result to a JSON file\n",
    "    with open('classification_result.json', 'w') as outfile:\n",
    "        json.dump(classification_result, outfile, indent=4)\n",
    "\n",
    "    print(\"Classification complete! Results saved to classification_result.json.\")\n",
    "\n",
    "\n",
    "# Exemple d'utilisation : traiter seulement les 100 premiers labels\n",
    "classify_images(json_content, max_labels=100)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-16T23:12:13.308311Z",
     "start_time": "2024-02-16T23:11:45.087501900Z"
    }
   },
   "id": "ac44a9fa1d74a9a7"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'classification_result-testSet.json'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[8], line 21\u001B[0m\n\u001B[0;32m     18\u001B[0m os\u001B[38;5;241m.\u001B[39mmakedirs(chemin_dossier_femmes, exist_ok\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[0;32m     20\u001B[0m \u001B[38;5;66;03m# Charger les données JSON\u001B[39;00m\n\u001B[1;32m---> 21\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28;43mopen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mchemin_fichier_json\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mr\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mas\u001B[39;00m fichier:\n\u001B[0;32m     22\u001B[0m     data \u001B[38;5;241m=\u001B[39m json\u001B[38;5;241m.\u001B[39mload(fichier)\n\u001B[0;32m     24\u001B[0m \u001B[38;5;66;03m# Déplacer les images dans les dossiers correspondants\u001B[39;00m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:310\u001B[0m, in \u001B[0;36m_modified_open\u001B[1;34m(file, *args, **kwargs)\u001B[0m\n\u001B[0;32m    303\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m file \u001B[38;5;129;01min\u001B[39;00m {\u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m2\u001B[39m}:\n\u001B[0;32m    304\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m    305\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mIPython won\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mt let you open fd=\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfile\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m by default \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    306\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    307\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124myou can use builtins\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m open.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    308\u001B[0m     )\n\u001B[1;32m--> 310\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mio_open\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfile\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: 'classification_result-testSet.json'"
     ]
    }
   ],
   "source": [
    "# trierHommeFemme.py\n",
    "# Permet de prendre un fichier json en entré comprennant deux labels (ex: homme et femme), puis il va trier les images dans les bons répertoires.\n",
    "\n",
    "import json\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "# Chemin du fichier JSON\n",
    "chemin_fichier_json = 'classification_result-testSet.json' # CHANGER LE NOM DU FICHIER POUR TRIER LES DIFFERENT SET. \n",
    "# classification_result-trainSet.json\n",
    "# classification_result-validationSet.json\n",
    "\n",
    "# Chemins des dossiers source et cible\n",
    "chemin_dossier_source = pathImageCelib\n",
    "chemin_dossier_hommes = pathTestSet+'/hommes/'\n",
    "chemin_dossier_femmes = pathTestSet+'/femmes/'\n",
    "\n",
    "# Créer les dossiers cibles s'ils n'existent pas\n",
    "os.makedirs(chemin_dossier_hommes, exist_ok=True)\n",
    "os.makedirs(chemin_dossier_femmes, exist_ok=True)\n",
    "\n",
    "# Charger les données JSON\n",
    "with open(chemin_fichier_json, 'r') as fichier:\n",
    "    data = json.load(fichier)\n",
    "\n",
    "# Déplacer les images dans les dossiers correspondants\n",
    "for homme in data['men']:\n",
    "    chemin_source = os.path.join(chemin_dossier_source, homme)\n",
    "    chemin_destination = os.path.join(chemin_dossier_hommes, homme)\n",
    "    shutil.move(chemin_source, chemin_destination)\n",
    "\n",
    "for femme in data['women']:\n",
    "    chemin_source = os.path.join(chemin_dossier_source, femme)\n",
    "    chemin_destination = os.path.join(chemin_dossier_femmes, femme)\n",
    "    shutil.move(chemin_source, chemin_destination)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-16T23:13:20.357310800Z",
     "start_time": "2024-02-16T23:13:20.297817300Z"
    }
   },
   "id": "22e0de9a61dc85ee"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1. Préparation des Données\n",
    "\n",
    "### 1.2 Augmentation des Données : Pour éviter le surapprentissage et améliorer la généralisation, surtout si vous disposez de peu de données, utilisez des techniques d'augmentation d'images (comme la rotation, le zoom, le décalage horizontal/vertical, etc.)."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e2cf6230b3f81441"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2205 images belonging to 2 classes.\n",
      "Found 2137 images belonging to 2 classes.\n",
      "Found 2058 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Configuration de l'augmentation des données\n",
    "datagen = ImageDataGenerator(\n",
    "    rescale=1./255,         # Normalisation des valeurs de pixels\n",
    "    rotation_range=40,      # Rotation aléatoire de l'image (degrés, 0-180)\n",
    "    width_shift_range=0.2,  # Décalage horizontal aléatoire (fraction de la largeur totale)\n",
    "    height_shift_range=0.2, # Décalage vertical aléatoire (fraction de la hauteur totale)\n",
    "    shear_range=0.2,        # Cisaillement aléatoire\n",
    "    zoom_range=0.2,         # Zoom aléatoire à l'intérieur des images\n",
    "    horizontal_flip=True,   # Retournement aléatoire des images horizontalement\n",
    "    fill_mode='nearest'     # Stratégie pour remplir les pixels nouvellement créés\n",
    ")\n",
    "\n",
    "# Chemin vers le dossier d'entraînement\n",
    "train_dir = pathTrainSet+'/'  # Mettez à jour avec le chemin approprié\n",
    "\n",
    "# Générateur de données d'entraînement\n",
    "train_generator = datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(178, 218),  # Taille des images après redimensionnement\n",
    "    batch_size=32,\n",
    "    class_mode='binary'  # 'binary' pour classification binaire, 'categorical' pour multiclasse\n",
    ")\n",
    "\n",
    "# Chemin vers le dossier de validation\n",
    "validation_dir = pathValidationSet+'/'  # Mettez à jour avec le chemin approprié\n",
    "\n",
    "# Générateur de données de validation\n",
    "validation_generator = datagen.flow_from_directory(\n",
    "    validation_dir,\n",
    "    target_size=(178, 218),  # Taille des images après redimensionnement\n",
    "    batch_size=32,\n",
    "    class_mode='binary'  # 'binary' pour classification binaire, 'categorical' pour multiclasse\n",
    ")\n",
    "\n",
    "# Chemin vers le dossier de test\n",
    "test_dir = pathTestSet+'/'  # Mettez à jour avec le chemin approprié\n",
    "\n",
    "# Générateur de données de test\n",
    "test_generator = datagen.flow_from_directory(\n",
    "    test_dir,\n",
    "    target_size=(178, 218),  # Taille des images après redimensionnement\n",
    "    batch_size=32,\n",
    "    class_mode='binary'  # 'binary' pour classification binaire, 'categorical' pour multiclasse\n",
    ")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-16T23:20:23.830817600Z",
     "start_time": "2024-02-16T23:20:23.668072400Z"
    }
   },
   "id": "78e1182684de4722"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. Construction du Modèle\n",
    "### 2.1 - Choix du Modèle : Vous pouvez construire un modèle de classification d'images à partir de zéro ou utiliser le transfert d'apprentissage à partir d'un modèle pré-entraîné.\n",
    "### 2.2 - Architecture du Modèle : Définissez l'architecture de votre modèle (couches, neurones, fonctions d'activation, etc.).\n",
    "### 2.3 - Compilation du Modèle : Compilez le modèle avec un optimiseur approprié, une fonction de perte et des métriques.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9e32f6ee3f8a0384"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\hydre\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "WARNING:tensorflow:From C:\\Users\\hydre\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\pooling\\max_pooling2d.py:161: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "WARNING:tensorflow:From C:\\Users\\hydre\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n"
     ]
    }
   ],
   "source": [
    "# model-perso.py\n",
    "# 2.1) Création d'un model à partir de zéro\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "\n",
    "### 2.2 - Architecture du Modèle\n",
    "model = Sequential([\n",
    "    Conv2D(32, (3, 3), activation='relu', input_shape=(178, 218, 3)),\n",
    "    MaxPooling2D(2, 2),\n",
    "    Conv2D(64, (3, 3), activation='relu'),\n",
    "    MaxPooling2D(2, 2),\n",
    "    Flatten(),\n",
    "    Dense(512, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(1, activation='sigmoid')  # 1 pour classification binaire (homme/femme)\n",
    "])\n",
    "\n",
    "### 2.3 - Compilation du Modèle\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-16T23:20:31.361086700Z",
     "start_time": "2024-02-16T23:20:30.311292200Z"
    }
   },
   "id": "b63346aa638a5355"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# model-précrée.py\n",
    "\n",
    "# # Utilisation d'un model déjà existant, on vient juste rajouter des couches\n",
    "# \n",
    "# from keras.applications.vgg16 import VGG16\n",
    "# from keras.models import Model\n",
    "# from keras.layers import Dense, GlobalAveragePooling2D\n",
    "# \n",
    "# # Charger VGG16 sans la partie supérieure (top) - sans les couches de classification\n",
    "# base_model = VGG16(weights='imagenet', include_top=False, input_shape=(150, 150, 3))\n",
    "# \n",
    "# # Ajouter vos propres couches pour la classification\n",
    "# x = base_model.output\n",
    "# x = GlobalAveragePooling2D()(x)\n",
    "# x = Dense(1024, activation='relu')(x)\n",
    "# predictions = Dense(1, activation='sigmoid')(x)  # 1 pour classification binaire\n",
    "# \n",
    "# model = Model(inputs=base_model.input, outputs=predictions)\n",
    "# \n",
    "# # Compiler le modèle\n",
    "# for layer in base_model.layers:\n",
    "#     layer.trainable = False  # Geler les couches du modèle pré-entraîné\n",
    "# \n",
    "# model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "1a1efbb19f8e46f7"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3. Entraînement du Modèle\n",
    "### 3.1 - Entraînement sur les Données : Entraînez votre modèle sur les données préparées, en utilisant éventuellement la validation croisée pour évaluer sa performance.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d167582a4ba7bb91"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "WARNING:tensorflow:From C:\\Users\\hydre\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "WARNING:tensorflow:From C:\\Users\\hydre\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      " 7/69 [==>...........................] - ETA: 42s - loss: 4.6855 - accuracy: 0.4732"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[11], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# 3.1) Entraînement sur les données\u001B[39;00m\n\u001B[1;32m----> 2\u001B[0m history \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m      3\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtrain_generator\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m      4\u001B[0m \u001B[43m    \u001B[49m\u001B[43msteps_per_epoch\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m69\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Nombre de lots à utiliser pendant chaque époque \u001B[39;49;00m\n\u001B[0;32m      5\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;66;43;03m# si vous avez 2205 images et un batch_size de 32, vous auriez environ 69 steps par époque (2205 / 32 = 68.9)\u001B[39;49;00m\n\u001B[0;32m      6\u001B[0m \u001B[43m    \u001B[49m\u001B[43mepochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m15\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m           \u001B[49m\u001B[38;5;66;43;03m# Nombre d'époques pour l'entraînement\u001B[39;49;00m\n\u001B[0;32m      7\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;66;43;03m# Si vous avez spécifié 100 steps_per_epoch et que vous voulez entraîner le modèle pour 15 époques, votre générateur doit pouvoir fournir (100 * 15 = 1500) lots au total.\u001B[39;49;00m\n\u001B[0;32m      8\u001B[0m \u001B[43m    \u001B[49m\u001B[43mvalidation_data\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mvalidation_generator\u001B[49m\u001B[43m,\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Si vous avez un générateur de validation\u001B[39;49;00m\n\u001B[0;32m      9\u001B[0m \u001B[43m    \u001B[49m\u001B[43mvalidation_steps\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m50\u001B[39;49m\u001B[43m   \u001B[49m\u001B[38;5;66;43;03m# Nombre de lots à utiliser pour la validation\u001B[39;49;00m\n\u001B[0;32m     10\u001B[0m \u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:65\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m     63\u001B[0m filtered_tb \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m     64\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m---> 65\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     66\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m     67\u001B[0m     filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\training.py:1807\u001B[0m, in \u001B[0;36mModel.fit\u001B[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001B[0m\n\u001B[0;32m   1799\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m tf\u001B[38;5;241m.\u001B[39mprofiler\u001B[38;5;241m.\u001B[39mexperimental\u001B[38;5;241m.\u001B[39mTrace(\n\u001B[0;32m   1800\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtrain\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m   1801\u001B[0m     epoch_num\u001B[38;5;241m=\u001B[39mepoch,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1804\u001B[0m     _r\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m,\n\u001B[0;32m   1805\u001B[0m ):\n\u001B[0;32m   1806\u001B[0m     callbacks\u001B[38;5;241m.\u001B[39mon_train_batch_begin(step)\n\u001B[1;32m-> 1807\u001B[0m     tmp_logs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain_function\u001B[49m\u001B[43m(\u001B[49m\u001B[43miterator\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1808\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m data_handler\u001B[38;5;241m.\u001B[39mshould_sync:\n\u001B[0;32m   1809\u001B[0m         context\u001B[38;5;241m.\u001B[39masync_wait()\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    148\u001B[0m filtered_tb \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m    149\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 150\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    151\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m    152\u001B[0m   filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:832\u001B[0m, in \u001B[0;36mFunction.__call__\u001B[1;34m(self, *args, **kwds)\u001B[0m\n\u001B[0;32m    829\u001B[0m compiler \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mxla\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jit_compile \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnonXla\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    831\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m OptionalXlaContext(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jit_compile):\n\u001B[1;32m--> 832\u001B[0m   result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwds\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    834\u001B[0m new_tracing_count \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mexperimental_get_tracing_count()\n\u001B[0;32m    835\u001B[0m without_tracing \u001B[38;5;241m=\u001B[39m (tracing_count \u001B[38;5;241m==\u001B[39m new_tracing_count)\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:868\u001B[0m, in \u001B[0;36mFunction._call\u001B[1;34m(self, *args, **kwds)\u001B[0m\n\u001B[0;32m    865\u001B[0m   \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_lock\u001B[38;5;241m.\u001B[39mrelease()\n\u001B[0;32m    866\u001B[0m   \u001B[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001B[39;00m\n\u001B[0;32m    867\u001B[0m   \u001B[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001B[39;00m\n\u001B[1;32m--> 868\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mtracing_compilation\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcall_function\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    869\u001B[0m \u001B[43m      \u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkwds\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_no_variable_creation_config\u001B[49m\n\u001B[0;32m    870\u001B[0m \u001B[43m  \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    871\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_variable_creation_config \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    872\u001B[0m   \u001B[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001B[39;00m\n\u001B[0;32m    873\u001B[0m   \u001B[38;5;66;03m# in parallel.\u001B[39;00m\n\u001B[0;32m    874\u001B[0m   \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_lock\u001B[38;5;241m.\u001B[39mrelease()\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compilation.py:139\u001B[0m, in \u001B[0;36mcall_function\u001B[1;34m(args, kwargs, tracing_options)\u001B[0m\n\u001B[0;32m    137\u001B[0m bound_args \u001B[38;5;241m=\u001B[39m function\u001B[38;5;241m.\u001B[39mfunction_type\u001B[38;5;241m.\u001B[39mbind(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    138\u001B[0m flat_inputs \u001B[38;5;241m=\u001B[39m function\u001B[38;5;241m.\u001B[39mfunction_type\u001B[38;5;241m.\u001B[39munpack_inputs(bound_args)\n\u001B[1;32m--> 139\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunction\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_flat\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# pylint: disable=protected-access\u001B[39;49;00m\n\u001B[0;32m    140\u001B[0m \u001B[43m    \u001B[49m\u001B[43mflat_inputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcaptured_inputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfunction\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcaptured_inputs\u001B[49m\n\u001B[0;32m    141\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\concrete_function.py:1323\u001B[0m, in \u001B[0;36mConcreteFunction._call_flat\u001B[1;34m(self, tensor_inputs, captured_inputs)\u001B[0m\n\u001B[0;32m   1319\u001B[0m possible_gradient_type \u001B[38;5;241m=\u001B[39m gradients_util\u001B[38;5;241m.\u001B[39mPossibleTapeGradientTypes(args)\n\u001B[0;32m   1320\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (possible_gradient_type \u001B[38;5;241m==\u001B[39m gradients_util\u001B[38;5;241m.\u001B[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001B[0;32m   1321\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m executing_eagerly):\n\u001B[0;32m   1322\u001B[0m   \u001B[38;5;66;03m# No tape is watching; skip to running the function.\u001B[39;00m\n\u001B[1;32m-> 1323\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_inference_function\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcall_preflattened\u001B[49m\u001B[43m(\u001B[49m\u001B[43margs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1324\u001B[0m forward_backward \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_select_forward_and_backward_functions(\n\u001B[0;32m   1325\u001B[0m     args,\n\u001B[0;32m   1326\u001B[0m     possible_gradient_type,\n\u001B[0;32m   1327\u001B[0m     executing_eagerly)\n\u001B[0;32m   1328\u001B[0m forward_function, args_with_tangents \u001B[38;5;241m=\u001B[39m forward_backward\u001B[38;5;241m.\u001B[39mforward()\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:216\u001B[0m, in \u001B[0;36mAtomicFunction.call_preflattened\u001B[1;34m(self, args)\u001B[0m\n\u001B[0;32m    214\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcall_preflattened\u001B[39m(\u001B[38;5;28mself\u001B[39m, args: Sequence[core\u001B[38;5;241m.\u001B[39mTensor]) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Any:\n\u001B[0;32m    215\u001B[0m \u001B[38;5;250m  \u001B[39m\u001B[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001B[39;00m\n\u001B[1;32m--> 216\u001B[0m   flat_outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcall_flat\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    217\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfunction_type\u001B[38;5;241m.\u001B[39mpack_output(flat_outputs)\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:251\u001B[0m, in \u001B[0;36mAtomicFunction.call_flat\u001B[1;34m(self, *args)\u001B[0m\n\u001B[0;32m    249\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m record\u001B[38;5;241m.\u001B[39mstop_recording():\n\u001B[0;32m    250\u001B[0m   \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_bound_context\u001B[38;5;241m.\u001B[39mexecuting_eagerly():\n\u001B[1;32m--> 251\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_bound_context\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcall_function\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    252\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    253\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mlist\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43margs\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    254\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mlen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfunction_type\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mflat_outputs\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    255\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    256\u001B[0m   \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    257\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m make_call_op_in_graph(\n\u001B[0;32m    258\u001B[0m         \u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m    259\u001B[0m         \u001B[38;5;28mlist\u001B[39m(args),\n\u001B[0;32m    260\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_bound_context\u001B[38;5;241m.\u001B[39mfunction_call_options\u001B[38;5;241m.\u001B[39mas_attrs(),\n\u001B[0;32m    261\u001B[0m     )\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\eager\\context.py:1486\u001B[0m, in \u001B[0;36mContext.call_function\u001B[1;34m(self, name, tensor_inputs, num_outputs)\u001B[0m\n\u001B[0;32m   1484\u001B[0m cancellation_context \u001B[38;5;241m=\u001B[39m cancellation\u001B[38;5;241m.\u001B[39mcontext()\n\u001B[0;32m   1485\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m cancellation_context \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m-> 1486\u001B[0m   outputs \u001B[38;5;241m=\u001B[39m \u001B[43mexecute\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mexecute\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1487\u001B[0m \u001B[43m      \u001B[49m\u001B[43mname\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdecode\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mutf-8\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1488\u001B[0m \u001B[43m      \u001B[49m\u001B[43mnum_outputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mnum_outputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1489\u001B[0m \u001B[43m      \u001B[49m\u001B[43minputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtensor_inputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1490\u001B[0m \u001B[43m      \u001B[49m\u001B[43mattrs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mattrs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1491\u001B[0m \u001B[43m      \u001B[49m\u001B[43mctx\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1492\u001B[0m \u001B[43m  \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1493\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m   1494\u001B[0m   outputs \u001B[38;5;241m=\u001B[39m execute\u001B[38;5;241m.\u001B[39mexecute_with_cancellation(\n\u001B[0;32m   1495\u001B[0m       name\u001B[38;5;241m.\u001B[39mdecode(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mutf-8\u001B[39m\u001B[38;5;124m\"\u001B[39m),\n\u001B[0;32m   1496\u001B[0m       num_outputs\u001B[38;5;241m=\u001B[39mnum_outputs,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1500\u001B[0m       cancellation_manager\u001B[38;5;241m=\u001B[39mcancellation_context,\n\u001B[0;32m   1501\u001B[0m   )\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001B[0m, in \u001B[0;36mquick_execute\u001B[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001B[0m\n\u001B[0;32m     51\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m     52\u001B[0m   ctx\u001B[38;5;241m.\u001B[39mensure_initialized()\n\u001B[1;32m---> 53\u001B[0m   tensors \u001B[38;5;241m=\u001B[39m \u001B[43mpywrap_tfe\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mTFE_Py_Execute\u001B[49m\u001B[43m(\u001B[49m\u001B[43mctx\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_handle\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice_name\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mop_name\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     54\u001B[0m \u001B[43m                                      \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mattrs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_outputs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     55\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m core\u001B[38;5;241m.\u001B[39m_NotOkStatusException \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m     56\u001B[0m   \u001B[38;5;28;01mif\u001B[39;00m name \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "# 3.1) Entraînement sur les données\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=69,  # Nombre de lots à utiliser pendant chaque époque \n",
    "    # si vous avez 2205 images et un batch_size de 32, vous auriez environ 69 steps par époque (2205 / 32 = 68.9)\n",
    "    epochs=15,           # Nombre d'époques pour l'entraînement\n",
    "    # Si vous avez spécifié 100 steps_per_epoch et que vous voulez entraîner le modèle pour 15 époques, votre générateur doit pouvoir fournir (100 * 15 = 1500) lots au total.\n",
    "    validation_data=validation_generator,  # Si vous avez un générateur de validation\n",
    "    validation_steps=50   # Nombre de lots à utiliser pour la validation\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-16T23:20:50.941835400Z",
     "start_time": "2024-02-16T23:20:43.211610700Z"
    }
   },
   "id": "e037a759ec9f97a7"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.2 - Callbacks : Utilisez des callbacks pour des fonctionnalités telles que l'enregistrement des meilleures performances, la réduction du taux d'apprentissage, etc."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ed1da8b2ad367cf9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "# Enregistrement du meilleur modèle\n",
    "checkpoint = ModelCheckpoint(\n",
    "    'meilleur_modele.h5',  # Nom du fichier pour sauvegarder le modèle\n",
    "    monitor='val_accuracy', # Métrique à surveiller\n",
    "    verbose=1,             # Afficher des messages détaillés\n",
    "    save_best_only=True,   # Sauvegarder uniquement le meilleur modèle\n",
    "    mode='max'             # Mode 'max' pour la métrique 'accuracy'\n",
    ")\n",
    "\n",
    "# Arrêt anticipé\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=10            # Nombre d'époques sans amélioration après lesquelles l'entraînement est arrêté\n",
    ")\n",
    "\n",
    "# Réduction du taux d'apprentissage\n",
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.2,            # Facteur de réduction du taux d'apprentissage\n",
    "    patience=5,            # Nombre d'époques sans amélioration avant la réduction\n",
    "    min_lr=0.001           # Taux d'apprentissage minimal\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=69,\n",
    "    epochs=15,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=50,\n",
    "    callbacks=[checkpoint, early_stopping, reduce_lr]\n",
    ")\n"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "be01ac2f6a8ee5e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4. Évaluation et Ajustement\n",
    "### 4.1 - Évaluer la Performance : Évaluez les performances de votre modèle sur un ensemble de données de test pour vérifier sa généralisation.\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2987671fd4d8fee0"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test_loss, test_accuracy = model.evaluate(test_generator)\n",
    "print(f\"Test accuracy: {test_accuracy:.4f}, Test loss: {test_loss:.4f}\")"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "ec603e825492da0c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 4.2 - Ajustement des Hyperparamètres : Ajustez les hyperparamètres au besoin pour améliorer les performances.\n",
    "\n",
    "Si les performances de votre modèle sur l'ensemble de test ne sont pas satisfaisantes, vous pourriez avoir besoin d'ajuster les hyperparamètres. Les hyperparamètres sont des paramètres qui ne sont pas appris pendant l'entraînement mais qui sont définis à l'avance et qui influencent le comportement et la performance du modèle d'apprentissage. Voici quelques hyperparamètres courants que vous pouvez ajuster :\n",
    "\n",
    "- **Taux d'apprentissage** : C'est un des hyperparamètres les plus importants. S'il est trop élevé, l'entraînement peut ne pas converger ; s'il est trop bas, l'entraînement peut être très lent ou se coincer dans un minimum local.\n",
    "- **Taille des lots** (batch size) : Une plus grande taille de lot peut conduire à une convergence plus stable et plus rapide, mais aussi à une généralisation moins bonne et à des exigences de mémoire plus élevées.\n",
    "- **Architecture du modèle** : Ajouter ou supprimer des couches, changer le nombre de neurones, essayer différentes fonctions d'activation, etc.\n",
    "- **Régularisation** : Techniques comme dropout ou L1/L2 pour prévenir le surajustement.\n",
    "- **Optimiseur** : Changer l'optimiseur (Adam, SGD, RMSprop, etc.) peut affecter la vitesse et la qualité de l'apprentissage.\n",
    "\n",
    "Après avoir ajusté les hyperparamètres, vous devrez réentraîner votre modèle et le réévaluer pour voir si les performances se sont améliorées.\n",
    "\n",
    "N'oubliez pas de noter les performances de chaque configuration pour pouvoir comparer et choisir la meilleure. Des outils comme TensorBoard peuvent être très utiles pour suivre les performances et les hyperparamètres pendant l'entraînement."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "479d4f1bf6874a6e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 5. Utilisation du Modèle\n",
    "### 5.1 - Sauvegarde du Modèle : Sauvegardez votre modèle entraîné pour une utilisation future."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "47ee787742f18b7a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Sauvegarde du modèle complet\n",
    "model.save('mon_modele.h5')  # Crée un fichier HDF5 'mon_modele.h5'"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "b7e95f6dca89ca26"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 120ms/step\n",
      "men , with  93.9995527267456 % probability\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing import image\n",
    "from keras.models import load_model\n",
    "import json\n",
    "\n",
    "\n",
    "# Chargement du même modèle\n",
    "model = load_model('mon_modele.h5')\n",
    "\n",
    "# Liste pour stocker les résultats\n",
    "results = []\n",
    "\n",
    "# Chargement et prétraitement d'une image\n",
    "img_path = \"../Image/img_label_3.jpg\"\n",
    "img = image.load_img(img_path, target_size=(178, 218))\n",
    "img_tensor = image.img_to_array(img)  # Convertit l'image en tableau numpy\n",
    "img_tensor = np.expand_dims(img_tensor, axis=0)  # Ajoute une dimension pour le lot\n",
    "img_tensor /= 255.  # Normalisation si nécessaire (comme pendant l'entraînement)\n",
    "\n",
    "# Faire la prédiction\n",
    "predictions = model.predict(img_tensor)\n",
    "\n",
    "# Traiter les prédictions\n",
    "if predictions[0] > 0.75 :\n",
    "    predicted_class = 'men'\n",
    "    print(predicted_class, \", with \", predictions[0][0]*100, \"% probability\")\n",
    "else :\n",
    "    predicted_class = 'women'\n",
    "    print(predicted_class, \", with \", 100 - predictions[0][0]*100, \"% probability\")\n",
    "\n",
    "# Fonction pour charger les résultats existants\n",
    "def load_existing_results(filename):\n",
    "    try:\n",
    "        with open(filename, 'r') as f:\n",
    "            return json.load(f)\n",
    "    except FileNotFoundError:\n",
    "        return []  # Retourne une liste vide si le fichier n'existe pas\n",
    "    \n",
    "# Fonction pour vérifier si un résultat existe déjà et le mettre à jour\n",
    "def update_or_add_prediction(filename, predicted_class, confidence, results):\n",
    "    # Recherche d'un résultat existant avec le même nom de fichier\n",
    "    for result in results:\n",
    "        if result['Filename'] == filename:\n",
    "            # Mise à jour du résultat existant\n",
    "            result['PredictedClass'] = predicted_class\n",
    "            result['Confidence'] = confidence\n",
    "            return\n",
    "    # Ajout d'un nouveau résultat si aucun résultat existant n'a été trouvé\n",
    "    results.append({\n",
    "        \"Filename\": filename,\n",
    "        \"PredictedClass\": predicted_class,\n",
    "        \"Confidence\": confidence\n",
    "    })\n",
    "    \n",
    "# Charger les résultats existants\n",
    "results_file = 'prediction_model1.json'\n",
    "existing_results = load_existing_results(results_file)\n",
    "\n",
    "# Mettre à jour ou ajouter un résultat de prédiction\n",
    "update_or_add_prediction(img_path, predicted_class, float(predictions[0][0]), existing_results)\n",
    "\n",
    "# Enregistrement dans un fichier JSON avec les résultats mis à jour\n",
    "with open(results_file, 'w') as f:\n",
    "    json.dump(existing_results, f, indent=4)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-16T23:40:04.496598600Z",
     "start_time": "2024-02-16T23:40:02.593350100Z"
    }
   },
   "id": "b816b920a6e27444"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Plus le résultat se rapproche de 100 %, plus la probabilité que ce soit un homme est élevé. \n",
    "À l'inverse, plus le résultat se rapproche de 0%, plus la probabilité d'être un homme est faible, et donc celle d'être une femme est élevé.\n",
    "\n",
    "Afin de mieux présenter le résultat, je soustrais le résultat à 100 dans le cas où le programme détecterait que c'est une femme, ce qui me donne aussi un pourcentage de certitude."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cf778605c413607c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 5.2 - Déploiement du Modèle : Déployez le modèle pour des prédictions en temps réel ou sur de nouvelles données.\n",
    "\n",
    "- **Application Web** : Nous avons créé une application en React afin d'afficher nos résultats et pouvoir les comparer plus facilement. \n",
    "\n",
    "Amélioration : On pourrait améliorer l'application afin d'afficher différents schémas qui nous aideraient à évaluer le modèle.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f90ed063156d50a5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-14T20:50:56.028242900Z",
     "start_time": "2024-02-14T20:50:55.999509Z"
    }
   },
   "id": "6c69fdaffd951a37"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
